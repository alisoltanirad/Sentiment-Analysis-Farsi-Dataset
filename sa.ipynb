{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "sa.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "metadata": {
        "id": "h3mRpqY8wIPb",
        "colab_type": "code",
        "outputId": "05ced9c1-11c9-49de-9262-920702ae2182",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 269
        }
      },
      "cell_type": "code",
      "source": [
        "import numpy\n",
        "import pandas\n",
        "import string\n",
        "from keras.datasets import imdb\n",
        "import re\n",
        "import nltk\n",
        "from nltk.stem.porter import PorterStemmer\n",
        "import ssl\n",
        "try:\n",
        "    _create_unverified_https_context = ssl._create_unverified_context\n",
        "except AttributeError:\n",
        "    pass\n",
        "else:\n",
        "    ssl._create_default_https_context = _create_unverified_https_context\n",
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from keras.preprocessing import sequence\n",
        "from keras import Sequential\n",
        "from keras.layers import Embedding, LSTM, Dense, Dropout\n",
        "\n",
        "\n",
        "vocabulary_size = 5000\n",
        "\n",
        "(X_train, y_train), (X_test, y_test) = imdb.load_data(num_words = vocabulary_size)\n",
        "\n",
        "TranslatedData = pandas.read_csv('https://raw.githubusercontent.com/alisoltanirad/Sentiment-Analysis-Farsi-Dataset/master/TranslatedDigikalaDataset.csv', sep=',')\n",
        "\n",
        "F_y = TranslatedData.iloc[0:719, 1].values\n",
        "\n",
        "F_X = []\n",
        "for i in range(719):\n",
        "    review = re.sub('[^a-zA-Z]', ' ', TranslatedData['Comment'][i])\n",
        "    review = review.lower()\n",
        "    review = review.split()\n",
        "    ps = PorterStemmer()\n",
        "    review = [ps.stem(word) for word in review if not word in set(stopwords.words('english'))]\n",
        "    review = ' '.join(review)\n",
        "    F_X.append(review)\n",
        "\n",
        "\n",
        "max_words = 500\n",
        "X_train = sequence.pad_sequences(X_train, maxlen=max_words)\n",
        "X_test = sequence.pad_sequences(X_test, maxlen=max_words)\n",
        "#F_X = sequence.pad_sequences(F_X, maxlen=max_words)\n",
        "cv = CountVectorizer(max_features=500)\n",
        "F_X = cv.fit_transform(F_X).toarray()\n",
        "\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_train, y_train, test_size=0.15, random_state=0)\n",
        "F_X_train, F_X_test, F_y_train, F_y_test = train_test_split(F_X, F_y, test_size=0.15, random_state=0)\n",
        "\n",
        "\n",
        "classifier = Sequential()\n",
        "classifier.add(Embedding(vocabulary_size, 200, input_length=max_words))\n",
        "classifier.add(LSTM(200))\n",
        "classifier.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "classifier.compile(loss='binary_crossentropy',\n",
        "                  optimizer='adam',\n",
        "                  metrics=['accuracy'])\n",
        "\n",
        "\n",
        "classifier.fit(X_train, y_train, validation_data=(X_test, y_test), batch_size=32, epochs=1, verbose=1)\n",
        "\n",
        "weights1 = classifier.layers[0].get_weights()\n",
        "weights2 = classifier.layers[1].get_weights()\n",
        "\n",
        "'''\n",
        "model = Sequential()\n",
        "model.load_weights('cache/vgg16_weights.h5')\n",
        "'''\n",
        "\n",
        "#classifier.layers.pop()\n",
        "#classifier.outputs = [classifier.layers[-1].output]\n",
        "#classifier.layers[-1].outbound_nodes = []\n",
        "\n",
        "#for layer in classifier.layers:\n",
        "#    layer.trainable = False\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Embedding(vocabulary_size, 200, input_length=max_words))\n",
        "model.add(LSTM(200))\n",
        "model.layers[0].set_weights(weights1)\n",
        "model.layers[1].set_weights(weights2)\n",
        "\n",
        "for layer in model.layers:\n",
        "    layer.trainable = False\n",
        "\n",
        "model.add(Dense(200, activation='relu'))\n",
        "\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "\n",
        "model.compile(loss='binary_crossentropy',\n",
        "                  optimizer='adam',\n",
        "                  metrics=['accuracy'])\n",
        "\n",
        "#valid_num = 54\n",
        "#F_X_valid, F_y_valid = F_X_train[:valid_num], F_y_train[:valid_num]\n",
        "#F_X_train, F_y_train = F_X_train[valid_num:], F_y_train[valid_num:]\n",
        "\n",
        "model.fit(F_X_train, F_y_train, batch_size=1, epochs=3, verbose=1)\n",
        "\n",
        "\n",
        "F_y_pred = model.predict(F_X_test)\n",
        "F_y_pred = (F_y_pred > 0.5)\n",
        "\n",
        "\n",
        "# Confusion Matrix\n",
        "cm = confusion_matrix(F_y_test, F_y_pred)\n",
        "accuracy = round(((cm[0][0]+cm[1][1])/(cm[0][0]+cm[0][1]+cm[1][0]+cm[1][1]))*100,3)\n",
        "print('\\n')\n",
        "print(accuracy)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "Train on 21250 samples, validate on 3750 samples\n",
            "Epoch 1/1\n",
            "21250/21250 [==============================] - 1282s 60ms/step - loss: 0.4860 - acc: 0.7644 - val_loss: 0.4482 - val_acc: 0.8053\n",
            "Epoch 1/3\n",
            "611/611 [==============================] - 36s 59ms/step - loss: 0.6848 - acc: 0.5581\n",
            "Epoch 2/3\n",
            "611/611 [==============================] - 35s 57ms/step - loss: 0.6829 - acc: 0.5745\n",
            "Epoch 3/3\n",
            "611/611 [==============================] - 35s 57ms/step - loss: 0.6831 - acc: 0.5777\n",
            "\n",
            "\n",
            "60.185\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "AtUrFaD-FrNh",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "FK5F5SHbwxdk",
        "colab_type": "code",
        "outputId": "3b784960-6c2d-43b7-d6f0-45907860d4e6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "cell_type": "code",
      "source": [
        "# Importing the libraries\n",
        "import numpy\n",
        "#import matplotlib.pyplot as plt\n",
        "import pandas\n",
        "#import re   #Cleaning the texts\n",
        "#import nltk\n",
        "#from nltk.stem.porter import PorterStemmer\n",
        "#import ssl\n",
        "\"\"\"\n",
        "try:\n",
        "    _create_unverified_https_context = ssl._create_unverified_context\n",
        "except AttributeError:\n",
        "    pass\n",
        "else:\n",
        "    ssl._create_default_https_context = _create_unverified_https_context\n",
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords\n",
        "\"\"\"\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "import io\n",
        "import requests\n",
        "import string\n",
        "\n",
        "# Importing the dataset\n",
        "dataset = pandas.read_csv(\"https://raw.githubusercontent.com/alisoltanirad/Sentiment-Analysis-Farsi-Dataset/master/DigikalaSentimentAnalysisDataset.csv\", sep=\",\")\n",
        "\n",
        "# Cleaning texts\n",
        "punctuations = set(string.punctuation)\n",
        "corpus = []\n",
        "for i in range(719):\n",
        "    data = str(dataset['Comment'][i])\n",
        "    data = ''.join((char for char in data if char not in punctuations))\n",
        "    corpus.append(data)\n",
        "\n",
        "\n",
        "# Creating the Bag of Words model\n",
        "cv = CountVectorizer(max_features=1500)\n",
        "X = cv.fit_transform(corpus).toarray()\n",
        "y = dataset.iloc[0:719, 1].values\n",
        "\n",
        "# Splitting the dataset into the Training set and Test set\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.25, random_state = 0)\n",
        "\n",
        "\n",
        "# Fitting classifier to the Training set\n",
        "classifier = RandomForestClassifier(n_estimators=10, criterion='entropy')\n",
        "classifier.fit(X_train, y_train)\n",
        "\n",
        "# Predicting the Test set results\n",
        "y_pred = classifier.predict(X_test)\n",
        "\n",
        "# Making the Confusion Matrix\n",
        "cm = confusion_matrix(y_test, y_pred)\n",
        "\n",
        "# Calculate Accuracy\n",
        "Accuracy = ((cm[0][0] + cm[1][1]) / (cm[0][0] + cm[1][1] + cm[0][1] + cm[1][0])) * 100\n",
        "print(Accuracy)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "78.33333333333333\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}